---
- name: Install client PostgreSQL
  become: yes
  apt:
    name: postgresql-client-16
    state: present
    update_cache: true

# DOWNLOAD AND EXTRACT Hive
- name: Download Hive 4.0.0 locally (if not exists)
  delegate_to: localhost
  become: false
  ansible.builtin.get_url:
    url: https://archive.apache.org/dist/hive/hive-4.0.0-alpha-2/apache-hive-4.0.0-alpha-2-bin.tar.gz
    dest: ./files/apache-hive-4.0.0-alpha-2-bin.tar.gz
    mode: '0644'
  when: not lookup('ansible.builtin.fileglob', './files/apache-hive-4.0.0-alpha-2-bin.tar.gz')

- name: Copy Hive
  become: yes
  ansible.builtin.copy:
    src: ./files/apache-hive-4.0.0-alpha-2-bin.tar.gz
    dest: /home/hadoop/apache-hive-4.0.0-alpha-2-bin.tar.gz
    owner: hadoop
    group: hadoop
    mode: '0644'

- name: Extract Hive
  become: yes
  ansible.builtin.unarchive:
    src: /home/hadoop/apache-hive-4.0.0-alpha-2-bin.tar.gz
    dest: /home/hadoop/
    remote_src: yes
    owner: hadoop
    group: hadoop

- name: Fix ownership of Hive directory
  become: yes
  ansible.builtin.file:
    path: /home/hadoop/apache-hive-4.0.0-alpha-2-bin
    state: directory
    recurse: yes
    owner: hadoop
    group: hadoop

- name: Download jdbc driver postgresql
  delegate_to: localhost
  become: false
  ansible.builtin.get_url:
    url: https://jdbc.postgresql.org/download/postgresql-42.7.4.jar
    dest: ./files/postgresql-42.7.4.jar
    mode: '0644'
  when: not lookup('ansible.builtin.fileglob', './files/postgresql-42.7.4.jar')  

- name: Copy jdbc driver postgresql
  become: yes
  ansible.builtin.copy:
    src: ./files/postgresql-42.7.4.jar
    dest: /home/hadoop/apache-hive-4.0.0-alpha-2-bin/lib/postgresql-42.7.4.jar
    owner: hadoop
    group: hadoop
    mode: '0644'

# Hive CONFIGURATION FILE
- name: Replace hive-site.xml
  become: yes
  ansible.builtin.template:
    src: hive-site.xml.j2
    dest: /home/hadoop/apache-hive-4.0.0-alpha-2-bin/conf/hive-site.xml
    owner: hadoop
    group: hadoop
    mode: '0644'

- name: Add Hive environment variables to .profile
  become: yes
  ansible.builtin.blockinfile:
    path: /home/hadoop/.profile
    owner: hadoop
    group: hadoop
    create: yes
    mode: '0644'
    block: |
      export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
      export HADOOP_HOME=/home/hadoop/hadoop-3.4.0
      export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
      export HIVE_HOME=/home/hadoop/apache-hive-4.0.0-alpha-2-bin
      export HIVE_CONF_DIR=$HIVE_HOME/conf
      export HIVE_AUX_JAR_PATH=$HIVE_HOME/lib/*
      export PATH=$PATH:$HIVE_HOME/bin

- name: Create hdfs dir for hive
  become: yes
  ansible.builtin.shell: |
    su - hadoop -c "source /home/hadoop/.profile && hdfs dfs -mkdir -p /user/hive/warehouse && hdfs dfs -mkdir -p /tmp && hdfs dfs -chmod g+w /user/hive/warehouse && hdfs dfs -chmod g+w /tmp"

- name: Init metastore
  become: yes
  ansible.builtin.shell: |
    su - hadoop -c "source /home/hadoop/.profile && schematool -dbType postgres -initSchema"

- name: Start hive
  become: yes
  ansible.builtin.shell: |
    su - hadoop -c "source /home/hadoop/.profile && hive --hiveconf hive.server2.enable.doAs=false --hiveconf hive.security.authorization.enabled=false --service hiveserver2 1>> /tmp/hs2.log 2>> /tmp/hs2_e.log &"

- name: Wait for HiveServer2 to start
  wait_for:
    host: nn
    port: 5433
    timeout: 60
  delegate_to: localhost

- name: Start metastore
  become: yes
  ansible.builtin.shell: |
    su - hadoop -c "source /home/hadoop/.profile && hive --hiveconf hive.server2.enable.doAs=false --hiveconf hive.security.authorization.enabled=false --service metastore 1>> /tmp/hm.log 2>> /tmp/hm_e.log &"

- name: Connect to hive
  become: yes
  ansible.builtin.shell: |
    su - hadoop -c "source /home/hadoop/.profile && \
    beeline -u jdbc:hive2://nn:5433 -e 'SHOW DATABASES;'"
  register: beeline_connection
  ignore_errors: true

- name: Copy people-10000.zip
  copy:
    src: /tmp/people-10000.zip
    dest: /tmp/people-10000.zip
  become: yes

- name: Unzip CSV
  become: yes
  unarchive:
    src: /tmp/people-10000.zip
    dest: /tmp/
    remote_src: yes

- name: Create input directory
  become: yes
  shell: |
    su - hadoop -c "source /home/hadoop/.profile && \
    hdfs dfs -mkdir -p /input && \
    hdfs dfs -chmod g+w /input"

- name: Load people.csv to HDFS
  become: yes
  shell: |
    su - hadoop -c "source /home/hadoop/.profile && \
    hdfs dfs -put -f /tmp/people-10000.csv /input/"

- name: Check HDFS file
  become: yes
  shell: |
    su - hadoop -c "source /home/hadoop/.profile && \
    hdfs fsck /input/people-10000.csv"

- name: Create tables and load data
  become: yes
  shell: |
    su - hadoop -c "source /home/hadoop/.profile && \
    beeline -u jdbc:hive2://nn:5433 -e \"
      DROP TABLE IF EXISTS people;
      DROP TABLE IF EXISTS people_partitioned;

      CREATE TABLE IF NOT EXISTS people (
        Index INT,
        UserId STRING,
        FirstName STRING,
        LastName STRING,
        Sex STRING,
        Email STRING,
        Phone STRING,
        DateOfBirth STRING,
        JobTitle STRING
      )
      ROW FORMAT DELIMITED
      FIELDS TERMINATED BY ','
      STORED AS TEXTFILE
      TBLPROPERTIES ('skip.header.line.count'='1');

      LOAD DATA INPATH '/input/people-10000.csv' OVERWRITE INTO TABLE people;

      CREATE TABLE IF NOT EXISTS people_partitioned (
        Index INT,
        UserId STRING,
        FirstName STRING,
        LastName STRING,
        Email STRING,
        Phone STRING,
        DateOfBirth STRING,
        JobTitle STRING
      )
      PARTITIONED BY (Sex STRING)
      ROW FORMAT DELIMITED
      FIELDS TERMINATED BY ','
      STORED AS TEXTFILE;

      SET hive.exec.dynamic.partition=true;
      SET hive.exec.dynamic.partition.mode=nonstrict;

      INSERT OVERWRITE TABLE people_partitioned PARTITION (Sex)
      SELECT
        Index, UserId, FirstName, LastName, Email, Phone, DateOfBirth, JobTitle, Sex
      FROM people;

      SHOW PARTITIONS people_partitioned;
    \""
  register: hive_create

- debug:
    msg: "{{ hive_create.stdout_lines }}"


- name: Test requests for tables people, people_partitioned
  become: yes
  shell: |
    su - hadoop -c "source /home/hadoop/.profile && \
    beeline -u jdbc:hive2://nn:5433 -e \"
      SELECT count(*) AS total_people FROM people;
      SELECT Sex, count(*) AS count_per_sex FROM people_partitioned GROUP BY Sex;
    \""
  register: hive_check

- debug:
    msg: "{{ hive_check.stdout_lines }}"
